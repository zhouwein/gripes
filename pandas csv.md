Pandas csv shenanigans
======================
`read_csv`
--------
### Arguments you want:
1. `dtype="object"`: prevent silent conversion of numeric-like data to float/int. eg.
    * `001234` -> `1234` (leading zeros removed)
    * `1234` -> `1234.0` (integers converted to float because of the presence of blank/null values in the same column)
2. `keep_default_na=False`, `na_values=""`: prevent null-ish values from getting converted to nulls. This might be desired in certain cases, but if you don't know the exact list of null-ish values it can lead to confusing behavior. eg.
    * `N/A` -> `np.nan` or `""` (if written to csv)

3. `encoding=` or `from_excel=True`: specify encoding directly to prevent corruption of special/non-latin/accented characters. By default python uses utf-8, which works fine even if the actual encoding is something else (eg. windows-1252 aka. what excel uses by default), as long as there aren't special characters. It also works fine if the file is utf-8 with BOM (aka utf-8-sig), but it may result in the BOM appearing in the first column name. If you're fairly certain it's from excel (eg. generated by us), you can use `from_excel=True` (only available in the read_csv in utils.py) to auto-detect the encoding.

`to_csv`
------
### Arguments you want:
1. `index=False`: prevents the index column (often the row number) from being added to the csv
2. `quoting=csv.QUOTE_NONNUMERIC`: prevent special characters (eg. commas, newlines) from messing up the output. There are various edge cases that cause to_csv and read_csv to not [round-trip](https://en.wikipedia.org/wiki/Round-trip_format_conversion) successfully, eg.
    * If there's a newline character at the end of a cell value, the cell won't get quoted. This is bad because when read_csv (or excel) tries to read it, it thinks the line abruptly ended.
    * If there's a `CR` character (aka.`\r` or `\x0D`) in the cell value, the cell won't get quoted. This is bad because when read_csv (or excel) tries to read it, it thinks the line abruptly ended.  